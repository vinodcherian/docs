
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://sitename.example/DL/DL2/">
      
      
        <link rel="prev" href="../DL1/">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Introduction2 - Docs</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#from-deep-learning-mindmap" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Docs" class="md-header__button md-logo" aria-label="Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M19.7 34.5c16.3-6.8 35 .9 41.8 17.2L192 364.8 322.5 51.7c6.8-16.3 25.5-24 41.8-17.2s24 25.5 17.2 41.8l-160 384c-5 11.9-16.6 19.7-29.5 19.7s-24.6-7.8-29.5-19.7l-160-384c-6.8-16.3.9-35 17.2-41.8"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Introduction2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="blue"  aria-label="Light Mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Light Mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-orange"  aria-label="Dark Mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Dark Mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Docs" class="md-nav__button md-logo" aria-label="Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M19.7 34.5c16.3-6.8 35 .9 41.8 17.2L192 364.8 322.5 51.7c6.8-16.3 25.5-24 41.8-17.2s24 25.5 17.2 41.8l-160 384c-5 11.9-16.6 19.7-29.5 19.7s-24.6-7.8-29.5-19.7l-160-384c-6.8-16.3.9-35 17.2-41.8"/></svg>

    </a>
    Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Machine Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Machine Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ML/introML/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ML/types_of_ML/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Types of ML
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ML/evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluation of ML Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ML/Data_Science/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Science
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ML/model_comparision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ML Models Comparision
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Deep Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../DL1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Introduction2
    
  </span>
  

      </a>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="from-deep-learning-mindmap">From Deep Learning MindMap<a class="headerlink" href="#from-deep-learning-mindmap" title="Permanent link">&para;</a></h1>
<p>Here is the information from the sources, structured into a hierarchical tree with bullet points as requested:</p>
<ol>
<li><strong>Concepts</strong>
a. <strong>Input Layer</strong>
i. Comprised of multiple <strong>Real-Valued inputs</strong>.
ii. Each input must be <strong>linearly independent</strong> from each other.
b. <strong>Hidden Layers</strong>
i. Layers other than the input and output layers.
ii. A layer is the <strong>highest-level building block in deep learning</strong>.
iii. A layer usually receives weighted input, transforms it with mostly <strong>non-linear functions</strong>, and passes these values as output to the next layer.
c. <strong>Batch Normalization</strong>
i. Using <strong>mini-batches of examples</strong> is helpful.
ii. The gradient of the loss over a mini-batch is an estimate of the gradient over the training set, and its quality improves as the batch size increases.
iii. Computation over a batch can be much more efficient due to the <strong>parallelism afforded by modern computing platforms</strong>.</li>
<li><strong>Cost/Loss (Min) Objective (Max) Functions</strong>
a. <strong>Maximum Likelihood Estimation (MLE)</strong>
i. Many cost functions are the result of applying Maximum Likelihood, such as the <strong>Least Squares cost function</strong> and <strong>Cross-Entropy</strong>.
ii. The likelihood of a parameter value (or vector of parameter values), θ, given outcomes x, is equal to the probability (density) assumed for those observed outcomes given those parameter values.
iii. The <strong>natural logarithm of the likelihood function</strong>, called the <strong>log-likelihood</strong>, is more convenient to work with.</li>
<li>It can be used in place of the likelihood in maximum likelihood estimation because the logarithm is a monotonically increasing function, thus achieving its maximum value at the same points as the function itself.
iv. It selects the set of values of the model parameters that <strong>maximises the likelihood function</strong> for a fixed set of data and underlying statistical model.
v. Intuitively, this maximises the "<strong>agreement</strong>" of the selected model with the observed data.
vi. For discrete random variables, it maximises the probability of the observed data under the resulting distribution.
vii. Maximum-likelihood estimation gives a <strong>unified approach to estimation</strong>, which is well-defined in the case of the normal distribution and many other problems.
b. <strong>Cross-Entropy</strong>
i. Can be used to define the <strong>loss function in machine learning and optimization</strong>.
ii. The true probability <em>pᵢ</em> represents the <strong>true label</strong>.
iii. The given distribution <em>qᵢ</em> represents the <strong>predicted value</strong> of the current model.
iv. An example is the <strong>Cross-entropy error function used with logistic regression</strong>.
c. <strong>Quadratic</strong>
i. The use of a quadratic loss function is common, for example when using <strong>least squares techniques</strong>.
ii. It is often more <strong>mathematically tractable</strong> than other loss functions due to the properties of variances.
iii. It is <strong>symmetric</strong>: an error above the target causes the same loss as the same magnitude of error below the target.
iv. If the target is <em>t</em>, then a quadratic loss function is defined as a specific formula.
d. <strong>0-1 Loss</strong>
i. In statistics and decision theory, this is a <strong>frequently used loss function</strong>.
e. <strong>Hinge Loss</strong>
i. A loss function used for <strong>training classifiers</strong>.
ii. For an intended output <em>t</em> = ±1 and a classifier score <em>y</em>, the hinge loss of the prediction <em>y</em> is defined as a specific formula.
f. <strong>Exponential</strong>
g. <strong>Hellinger Distance</strong>
i. Used to <strong>quantify the similarity between two probability distributions</strong>.
ii. It is a type of <strong>f-divergence</strong>.
iii. The square of the Hellinger distance between two probability measures P and Q (absolutely continuous with respect to a third probability measure λ) is defined as a specific quantity.
h. <strong>Kullback-Leibler Divergence</strong>
i. Is a measure of <strong>how one probability distribution diverges from a second expected probability distribution</strong>.
ii. Applications include:</li>
<li>Characterising the <strong>relative (Shannon) entropy</strong> in information systems.</li>
<li>Randomness in continuous time-series.</li>
<li>Information gain when comparing statistical models of inference.
iii. Can be applied to <strong>Discrete</strong> or <strong>Continuous</strong> distributions.
i. <strong>Itakura–Saito distance</strong>
i. Is a measure of the <strong>difference between an original spectrum <em>P(ω)</em> and an approximation *P^(ω)</strong>* of that spectrum.
ii. Although not a perceptual measure, it is intended to reflect <strong>perceptual (dis)similarity</strong>.</li>
<li><strong>Regularization</strong>
a. <strong>L1 norm (Manhattan Distance)</strong>
i. Also known as <strong>least absolute deviations (LAD)</strong> or <strong>least absolute errors (LAE)</strong>.
ii. It is basically minimizing the sum of the absolute differences (S) between the target value and the estimated values.
b. <strong>L2 norm (Euclidean Distance)</strong>
i. Also known as <strong>least squares</strong>.
ii. It is basically minimizing the sum of the square of the differences (S) between the target value and the estimated values.
c. <strong>Early Stopping</strong>
i. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to <strong>over-fit</strong>, and stop the algorithm then.
ii. Use parameters that gave the <strong>best validation error</strong>.
d. <strong>Dropout</strong>
i. A <strong>regularization technique for reducing overfitting in neural networks</strong>.
ii. Prevents complex co-adaptations on training data.
iii. It is a very efficient way of performing model averaging with neural networks.
iv. The term "dropout" refers to <strong>dropping out units</strong> (both hidden and visible) in a neural network.
v. During <strong>training time</strong>: at each instance of evaluation (in online SGD-training), randomly set 50% of the inputs to each neuron to 0.
vi. During <strong>test time</strong>: halve the model weights.
vii. This prevents <strong>feature co-adaptation</strong>, meaning a feature cannot only be useful in the presence of particular other features.
e. <strong>Sparse regularizer on columns</strong>
i. This regularizer defines an <strong>L2 norm on each column</strong> and an <strong>L1 norm over all columns</strong>.
ii. It can be solved by <strong>proximal methods</strong>.
f. <strong>Nuclear norm regularization</strong>
g. <strong>Mean-constrained regularization</strong>
i. This regularizer constrains the functions learned for each task to be <strong>similar to the overall average</strong> of the functions across all tasks.
ii. This is useful for expressing prior information that each task is expected to <strong>share similarities</strong> with each other task.
iii. An example is predicting blood iron levels measured at different times of the day, where each task represents a different person.
iv. This regularizer is similar to the mean-constrained regularizer, but instead enforces a different constraint.</li>
<li><strong>Weight Initialization</strong>
a. <strong>All Zero Initialization</strong>
i. In the ideal situation, with proper data normalization, it is reasonable to assume that approximately half of the weights will be positive and half will be negative.
ii. Setting all initial weights to zero, expecting it to be the "best guess", turns out to be a <strong>mistake</strong>.
iii. If every neuron computes the same output, they will also compute the same gradients during back-propagation and undergo the exact same parameter updates, meaning there is <strong>no source of asymmetry between neurons</strong>.
b. <strong>Initialization with Small Random Numbers</strong>
i. Weights should be <strong>very close to zero, but not identically zero</strong>.
ii. Randomising neurons to small numbers (very close to zero) is treated as <strong>symmetry breaking</strong>.
iii. The idea is that neurons are initially random and unique, so they will compute distinct updates and integrate as diverse parts of the full network.
iv. Implementation for weights might simply involve drawing values from a <strong>normal distribution with zero mean, and unit standard deviation</strong>.
v. Using small numbers drawn from a <strong>uniform distribution</strong> is also possible, but has relatively little impact on final performance in practice.
c. <strong>Calibrating the Variances</strong>
i. A problem with small random number initialization is that the distribution of outputs from a randomly initialized neuron has a variance that <strong>grows with the number of inputs</strong>.
ii. This variance can be normalized to 1 by <strong>scaling each neuron's weight vector by the square root of its fan-in</strong> (its number of inputs).
iii. This ensures that all neurons initially have approximately the same output distribution.
iv. Empirically, this improves the <strong>rate of convergence</strong>.
v. The derivations for this can be found in pages 18 to 23 of the slides (not provided).
vi. Note that the derivations do not consider the influence of ReLU neurons.</li>
<li><strong>Optimization</strong>
a. <strong>Gradient Descent</strong>
i. A <strong>first-order iterative optimization algorithm</strong> for finding the minimum of a function.
ii. To find a local minimum, one takes steps <strong>proportional to the negative of the gradient</strong> (or approximate gradient) of the function at the current point.
iii. If steps are proportional to the positive of the gradient, one approaches a local maximum; this is known as <strong>gradient ascent</strong>.
iv. Uses the <strong>total gradient over all examples per update</strong>.
v. Ordinary gradient descent as a batch method is <strong>very slow</strong> and should never be used.
b. <strong>Stochastic Gradient Descent (SGD)</strong>
i. With SGD, training proceeds in steps, considering a <strong>mini-batch <em>x1...m</em> of size *m</strong>* at each step.
ii. The mini-batch is used to approximate the gradient of the loss function with respect to the parameters.
iii. Updates after <strong>only 1 or a few examples</strong>.
iv. On large datasets, <strong>SGD usually wins over all batch methods</strong>.
c. <strong>Mini-batch Stochastic Gradient Descent (SGD)</strong>
i. Most commonly used now.
ii. Size of each mini-batch <em>B</em> typically ranges from <strong>20 to 1000</strong>.
iii. Helps <strong>parallelising any model</strong> by computing gradients for multiple elements of the batch in parallel.
d. <strong>Momentum</strong>
i. Idea: <strong>Add a fraction <em>v</em> of previous update</strong> to the current one.
ii. When the gradient keeps pointing in the same direction, this will <strong>increase the size of the steps taken towards the minimum</strong>.
iii. Reduce global learning rate when using a lot of momentum.
iv. Update Rule: <em>v</em> is initialised at 0.
v. Momentum often increased after some epochs (e.g., from 0.5 to 0.99).
e. <strong>Adagrad</strong>
i. Provides <strong>adaptive learning rates for each parameter</strong>.
ii. The learning rate adapts differently for each parameter.
iii. <strong>Rare parameters get larger updates</strong> than frequently occurring parameters, which is useful for word vectors.
iv. It is a method for <strong>not hand-setting learning rates</strong>.
f. <strong>Learning Rate</strong>
i. Neural networks are often trained by gradient descent on the weights.
ii. At each iteration, backpropagation calculates the derivative of the loss function with respect to each weight, which is then subtracted from that weight.
iii. If the weights change too much each iteration, they will "<strong>overcorrect</strong>" and the loss will increase/diverge.
iv. In practice, people usually <strong>multiply each derivative by a small value called the “learning rate”</strong> before subtracting it from its corresponding weight.
v. <strong>Simplest recipe</strong>: keep it fixed and use the same for all parameters.
vi. <strong>Better results</strong> by allowing learning rates to decrease.</li>
<li>Options include: Reducing by 0.5 when validation error stops improving.</li>
<li>Reduction by O(1/t) because of theoretical convergence guarantees, with hyper-parameters ε0 and τ and <em>t</em> as iteration numbers.</li>
<li><strong>Backpropagation</strong>
a. A method used in artificial neural networks to <strong>calculate the error contribution of each neuron</strong> after a batch of data.
b. It calculates the <strong>gradient of the loss function</strong>.
c. It is commonly used in the <strong>gradient descent optimization algorithm</strong>.
d. Also called <strong>backward propagation of errors</strong>, because the error is calculated at the output and distributed back through the network layers.
e. It reuses partial derivatives computed for higher layers in lower layers, for <strong>efficiency</strong>.</li>
<li><strong>Activation Functions</strong>
a. Defines the <strong>output of that node given an input or set of inputs</strong>.
b. Types include:
i. <strong>ReLU</strong>
ii. <strong>Sigmoid / Logistic</strong>
iii. <strong>Binary</strong>
iv. <strong>Tanh</strong>
v. <strong>Softplus</strong>
vi. <strong>Softmax</strong>
vii. <strong>Maxout</strong>
viii. <strong>Leaky ReLU, PReLU, RReLU, ELU, SELU</strong>, and others.</li>
<li><strong>Architectures Strategy</strong>
a. <strong>Check for implementation bugs with gradient checks</strong>
i. Implement your gradient.
ii. Implement a <strong>finite difference computation</strong> by looping through the network parameters, adding and subtracting a small epsilon (10⁻⁴), and estimating derivatives.
iii. <strong>Compare the two</strong> and make sure they are almost the same.
iv. If your gradient fails and you don’t know why, <strong>simplify your model until you have no bug</strong>.
v. Create a <strong>very tiny synthetic model and dataset</strong>.
vi. Example progression from simplest to more complex model for debugging:</li>
<li>Only softmax on fixed input.</li>
<li>Backprop into word vectors and softmax.</li>
<li>Add single unit single hidden layer.</li>
<li>Add multi unit single layer.</li>
<li>Add second layer single unit, then multiple units, bias.</li>
<li>Add one softmax on top, then two softmax layers.</li>
<li>Add bias.
b. <strong>Parameter Initialization</strong>
i. Initialize <strong>hidden layer biases to 0</strong>.
ii. Initialize <strong>output (or reconstruction) biases to optimal value</strong> if weights were 0 (e.g., mean target or inverse sigmoid of mean target).
iii. Initialize <strong>weights Uniform(−r, r)</strong>, where <em>r</em> is inversely proportional to fan-in (previous layer size) and fan-out (next layer size).
c. <strong>Optimization</strong>
i. <strong>Gradient Descent</strong> (as described in section 5.a).
ii. <strong>Stochastic Gradient Descent (SGD)</strong> (as described in section 5.b).</li>
<li>Ordinary gradient descent as a batch method is very slow and <strong>should never be used</strong>; use 2nd order batch methods such as L-BFGS.</li>
<li>On large datasets, <strong>SGD usually wins</strong> over all batch methods.</li>
<li>On smaller datasets, L-BFGS or Conjugate Gradients win.</li>
<li>Large-batch L-BFGS extends the reach of L-BFGS.
iii. <strong>Mini-batch Stochastic Gradient Descent (SGD)</strong> (as described in section 5.c).
iv. <strong>Momentum</strong> (as described in section 5.d).
v. <strong>Adagrad</strong> (as described in section 5.e).
d. <strong>Check if the model is powerful enough to overfit</strong>
i. If not, <strong>change model structure</strong> or make the model "larger".
ii. If you can overfit, <strong>regularise to prevent overfitting</strong>.</li>
<li><strong>Simple first step</strong>: Reduce model size by lowering the number of units and layers and other parameters.</li>
<li>Apply <strong>Standard L1 or L2 regularization</strong> on weights.</li>
<li>Use <strong>Early Stopping</strong>: parameters that gave the best validation error.</li>
<li>Apply <strong>Sparsity constraints on hidden activations</strong>, e.g., by adding to the cost.</li>
<li>Implement <strong>Dropout</strong> (as described in section 3.d).</li>
<li><strong>Neural Network Architectures / Types</strong>
a. <strong>RNNs (Recursive)</strong>
i. A kind of deep neural network created by applying the same set of weights recursively over a structure.
ii. It produces a structured prediction over variable-size input structures, or a scalar prediction, by traversing a given structure in topological order.
iii. RNNs have been successful in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding.
b. <strong>RNNs (Recurrent)</strong>
i. A class of artificial neural network where <strong>connections between units form a directed cycle</strong>.
ii. This allows them to exhibit <strong>dynamic temporal behavior</strong>.
iii. Unlike feedforward neural networks, RNNs can use their <strong>internal memory to process arbitrary sequences of inputs</strong>.
iv. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.
v. <strong>LSTMs</strong> are a type of recurrent RNN.
c. <strong>Convolutional Neural Networks (CNN)</strong>
i. They have applications in <strong>image and video recognition, recommender systems, and natural language processing</strong>.
ii. Key components include:</li>
<li><strong>Pooling</strong></li>
<li><strong>Convolution</strong></li>
<li><strong>Subsampling</strong>
d. <strong>Auto-Encoders</strong>
i. An artificial neural network used for <strong>unsupervised learning of efficient codings</strong>.
ii. The aim is to learn a representation (encoding) for a set of data, typically for <strong>dimensionality reduction</strong>.
iii. Recently, the autoencoder concept has become more widely used for <strong>learning generative models of data</strong>.
e. <strong>GANs (Generative Adversarial Networks)</strong>
i. A class of artificial intelligence algorithms used in <strong>unsupervised machine learning</strong>.
ii. Implemented by a system of <strong>two neural networks contesting with each other in a zero-sum game framework</strong>.
f. <strong>LSTMs (Long Short-Term Memory)</strong>
i. It is a type of <strong>recurrent RNN</strong>.
ii. Allows data to flow both <strong>forwards and backwards</strong> within the network.
iii. An LSTM is well-suited to learn from experience to classify, process, and predict time series given time lags of unknown size and bound between important events.
iv. Relative insensitivity to gap length gives an advantage to LSTM over alternative RNNs, hidden Markov models, and other sequence learning methods in numerous applications.
g. <strong>Feed Forward</strong>
i. An artificial neural network wherein <strong>connections between the units do not form a cycle</strong>.
ii. Information moves in only one direction, <strong>forward</strong>, from the input nodes, through the hidden nodes (if any) and to the output nodes.
iii. There are no cycles or loops in the network.
iv. Kinds:</li>
<li><strong>Single-Layer Perceptron</strong>
a. The inputs are fed directly to the outputs via a series of weights.
b. By adding a <strong>Logistic activation function</strong> to the outputs, the model is identical to a classical Logistic Regression model.</li>
<li><strong>Multi-Layer Perceptron</strong>
a. This class of networks consists of multiple layers of computational units, usually interconnected in a <strong>feed-forward way</strong>.
b. Each neuron in one layer has directed connections to the neurons of the subsequent layer.
c. In many applications, the units of these networks apply a <strong>sigmoid function as an activation function</strong>.</li>
<li><strong>TensorFlow</strong>
a. <strong>Packages</strong>
i. <strong><code>tf</code></strong> (Main).
ii. <strong><code>tf.estimator</code></strong>: TensorFlow’s high-level machine learning API.</li>
<li>Makes it easy to configure, train, and evaluate a variety of machine learning models.</li>
<li>Includes pre-canned estimators like:
a. <code>tf.estimator.LinearClassifier</code>: Constructs a linear classification model.
b. <code>tf.estimator.LinearRegressor</code>: Constructs a linear regression model.
c. <code>tf.estimator.DNNClassifier</code>: Constructs a neural network classification model.
d. <code>tf.estimator.DNNRegressor</code>: Constructs a neural network regression model.
e. <code>tf.estimator.DNNLinearCombinedClassifier</code>: Constructs a neural network and linear combined classification model.
f. <code>tf.estimator.DNNRegressor</code>: Constructs a neural network and linear combined regression model.
b. <strong>Main Steps (General)</strong>
i. Create the Model.
ii. Define Target.
iii. Define Loss function and Optimizer.
iv. Define the Session and Initialise Variables.
v. Train the Model.
vi. Test Trained Model.
c. <strong>Main Steps (using <code>tf.estimator</code>)</strong>
i. <strong>Define Feature Columns</strong></li>
<li>These are the primary way of encoding features for pre-canned <code>tf.learn</code> Estimators.</li>
<li>The type of feature column chosen depends on the feature type and the model type.</li>
<li><strong>Continuous Features</strong> can be represented by <code>real_valued_column</code>.</li>
<li><strong>Categorical Features</strong> can be represented by any <code>sparse_column_with_*</code> column (e.g., <code>sparse_column_with_keys</code>, <code>sparse_column_with_vocabulary_file</code>, <code>sparse_column_with_hash_bucket</code>, <code>sparse_column_with_integerized_feature</code>).
ii. <strong>Define your Layers, or use a prebuilt model</strong> (e.g., a pre-built Logistic Regression Classifier).
iii. <strong>Write the <code>input_fn</code> function</strong></li>
<li>This function holds the actual data (features and labels).</li>
<li><code>Features</code> is a Python dictionary.
iv. <strong>Train the model</strong> using the <code>fit</code> function, on the <code>input_fn</code>.</li>
<li>The feature columns are fed to the model as arguments.
v. <strong>Predict and Evaluate</strong> using the <code>eval_input_fn</code> defined previously.
d. <strong>Comparison to Numpy</strong>
i. TensorFlow does <strong>lazy evaluation</strong>.
ii. You need to build the graph, and then run it in a session.
e. <strong>Main Components</strong>
i. <strong>Variables</strong></li>
<li><strong>Stateful nodes</strong> that output their current value, with their state retained across multiple executions of the graph.</li>
<li>Mostly <strong>parameters we’re interested in tuning</strong>, such as Weights (W) and Biases (b).</li>
<li>They are in-memory buffers containing tensors.</li>
<li>Declared variables must be initialised before they have values.</li>
<li>When training a model, variables are used to hold and update parameters.</li>
<li>Sharing variables can be done by:
a. Explicitly passing <code>tf.Variable</code> objects around.
b. Implicitly wrapping <code>tf.Variable</code> objects within <code>tf.variable_scope</code> objects.
ii. <strong>Scopes</strong></li>
<li><code>tf.variable_scope()</code>: Provides simple <strong>name spacing</strong> to avoid cases when querying.</li>
<li><code>tf.get_variable()</code>: Creates/Accesses variables from a variable scope.
iii. <strong>Placeholders</strong></li>
<li>Nodes whose <strong>value is fed at execution time</strong>.</li>
<li>Used for Inputs, Features (X) and Labels (y).
iv. <strong>Mathematical Operations</strong>: Examples include <code>MatMul</code>, <code>Add</code>, <code>ReLU</code>, etc..
v. <strong>Graph Nodes</strong>: They are Operations, containing any number of inputs and outputs.
vi. <strong>Edges</strong>: The tensors that flow between the nodes.
vii. <strong>Session</strong></li>
<li>It is a binding to a <strong>particular execution context</strong>: CPU, GPU.</li>
<li>A Session object encapsulates the environment in which Tensor objects are evaluated.</li>
<li>Uses a session to execute operations (<code>ops</code>) in the graph.</li>
<li>Running a Session involves:
a. <strong>Inputs</strong>.
b. <strong>Fetches</strong>: A list of graph nodes, returning the output of these nodes.
c. <strong>Feeds</strong>: A dictionary mapping from graph nodes to concrete values, specifying the value of each graph node given in the dictionary.
f. <strong>Phases</strong>
i. <strong>Construction</strong>: Assembles a <strong>computational graph</strong>.</li>
<li>The computation graph has no numerical value until evaluated.</li>
<li>All computations add nodes to the global default graph.
ii. <strong>Execution</strong>: A Session object encapsulates the environment in which Tensor objects are evaluated.</li>
<li>Uses a session to execute <code>ops</code> in the graph.
g. <strong>TensorBoard</strong>: TensorFlow has some neat <strong>built-in visualisation tools</strong>.
h. <strong>Intuition</strong>
i. Google provides primitives for defining functions on tensors and automatically computing their derivatives, <strong>expressed as a graph</strong>.
ii. The TensorFlow Graph is built to contain all placeholders for X and y, all variables for W’s and b’s, all mathematical operations, the cost function, and the optimisation procedure.
iii. At runtime, the values for the data are fed into that Graph, by placing the data batches in the placeholders and running the Graph.
iv. Each node in the Graph can then be connected to each other node over the network, allowing <strong>TensorFlow models to be parallelised</strong>.</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../DL1/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Introduction1">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Introduction1
              </div>
            </div>
          </a>
        
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2026 Name
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.instant.prefetch", "navigation.sections", "navigation.path", "header.autohide", "navigation.footer", "navigation.top", "toc.follow", "search.highlight", "search.suggest"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../javascripts/shortcuts.js"></script>
      
    
  </body>
</html>